{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc7cdbf-cf03-4f80-948c-1c2c7884306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this portion is done to ignore warnings from coffea for now\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import parse\n",
    "from atlas_schema.methods import behavior as as_behavior\n",
    "from atlas_schema.schema import NtupleSchema\n",
    "from coffea import processor\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea.dataset_tools import apply_to_fileset\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue.htcondor import HTCondorCluster\n",
    "from dask.distributed import LocalCluster\n",
    "from matplotlib import pyplot as plt\n",
    "import hist.dask as had\n",
    "\n",
    "fname_pattern = parse.compile(\n",
    "    \"user.{username:w}.{dsid:d}.{process:S}.{campaign:w}.v{version:.1f}_ANALYSIS.root\"\n",
    ")\n",
    "\n",
    "colors_dict = {\n",
    "    \"Znunu\": \"b\",\n",
    "    \"Wenu\": \"g\",\n",
    "    \"Wmunu\": \"r\",\n",
    "    \"Wtaunu_L\": \"c\",\n",
    "    \"Wtaunu_H\": \"m\",\n",
    "    \"Znunugamma\": \"y\",\n",
    "    \"Wmunugamma\": \"k\",\n",
    "    \"Wenugamma\": \"brown\",\n",
    "    \"Wtaunugamma\": \"pink\",\n",
    "    \"N2_100_N1_97_WB_signal\": \"rosybrown\",\n",
    "    \"Fake/Nonprompt\": \"lime\",\n",
    "}  #  'slategrey', 'blueviolet', 'crimson'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bab73c1-a262-459b-b2f7-d80859abf41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process(self, events):\n",
    "        ## TODO: remove this temporary fix when https://github.com/scikit-hep/vector/issues/498 is resolved\n",
    "        met_dict = {field: events.met[field] for field in events.met.fields}\n",
    "        met_dict[\"pt\"] = dak.zeros_like(events.met.met)\n",
    "        met_dict[\"eta\"] = dak.zeros_like(events.met.met)\n",
    "        events[\"met\"] = dak.zip(met_dict, with_name=\"MissingET\", behavior=as_behavior)\n",
    "\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        \n",
    "        print(f\"processing {len(events)} events for {dataset}\")\n",
    "        # xs = events.metadata[\"xs\"]\n",
    "        # lum = events.metadata[\"luminosity\"]\n",
    "        process = events.metadata[\"process\"]\n",
    "        # genFiltEff = events.metadata[\"genFiltEff\"]\n",
    "        # evt_count = ak.num(events, axis=0).compute()\n",
    "        # weights = (xs * genFiltEff * lum / evt_count) * np.ones(evt_count)\n",
    "\n",
    "        leptons = ak.concatenate((events.el, events.mu), axis=1)\n",
    "\n",
    "\n",
    "        # here are some selection cuts for something that looks like the signal region.\n",
    "        # the only thing that's different is the MET requirement, which I inverted to be\n",
    "        # met<250 instead of met>250, to make sure we don't accidentally unblind the SR\n",
    "        # and to give us some more stats while we study MC samples.\n",
    "        selections = {\n",
    "            \"met\": events.met.met < 250 * 1.0e3,\n",
    "            \"lepton_veto\": ak.num(leptons, axis=1) == 0,\n",
    "            \"leading_jet_pt\": ak.firsts(events.jet.pt) > 100 * 1.0e3,\n",
    "            \"leading_photon_pt\": ak.firsts(events.ph.pt) > 10 * 1.0e3,\n",
    "            \"min_dphi_jet_met\": ak.min(events.met.delta_phi(events.jet), axis=1) > 0.4,\n",
    "            \"min_dr_photon_jet\": ak.min(\n",
    "                ak.min(\n",
    "                    events.jet.metric_table(\n",
    "                        events.ph, metric=lambda a, b: a.delta_r(b)\n",
    "                    ),\n",
    "                    axis=2,\n",
    "                ),\n",
    "                axis=1,\n",
    "            ) > 0.4,\n",
    "            \"bjet_veto\": ak.sum(events.jet.btag_select, axis=1) == 0,\n",
    "            \"vgamma_overlap\": events[\"in\"][\n",
    "                \"vgamma_overlap_7\"\n",
    "            ],  # ['vgamma_overlap_7', 'vgamma_overlap_10', 'vgamma_overlap_15', 'vgamma_overlap_20']\n",
    "        }\n",
    "        \n",
    "        selection = PackedSelection()\n",
    "        selection.add_multiple(selections)\n",
    "\n",
    "        SR=(selection.all())\n",
    "\n",
    "        # preselection\n",
    "        photon_preselection = (\n",
    "            (events.ph.pt>10000) &\n",
    "            (events.ph.select_baseline==1) &\n",
    "            ((events.ph.isEM&0x45fc01)==0) &\n",
    "            ((abs(events.ph.eta)<1.37) | \n",
    "             (abs(events.ph.eta)>1.52) & \n",
    "              (abs(events.ph.eta)<2.37))\n",
    "        )\n",
    "        \n",
    "        photon_selections = {\n",
    "            \"tight\":  (events.ph.select_tightID  == 1),\n",
    "            \"ntight\": (events.ph.select_tightID  == 0),\n",
    "            \"iso\":    (events.ph.select_tightIso == 1),\n",
    "            \"niso\":   (events.ph.select_tightIso == 0),\n",
    "        }\n",
    "\n",
    "        truth_selections = {\n",
    "            \"prompt\": ((events.ph.truthpdgId == 22) & (events.ph.truthType != 16)),\n",
    "            \"fakenp\": ((events.ph.truthpdgId != 22) | (events.ph.truthType == 16)),\n",
    "        }\n",
    "\n",
    "        dataorfakemc=False\n",
    "        if dataorfakemc:\n",
    "            truthsel=truth_selections[\"fakenp\"]\n",
    "        else:\n",
    "            truthsel=truth_selections[\"prompt\"]\n",
    "\n",
    "        print(ak.type(SR))\n",
    "        print(ak.type(truthsel & photon_preselection & photon_selections[\"tight\"]  & photon_selections[\"iso\"] ))\n",
    "        \n",
    "        ABCD_selections = {\n",
    "            \"tight_iso\"  : ak.firsts(truthsel & photon_preselection & photon_selections[\"tight\"]  & photon_selections[\"iso\"] ),\n",
    "            \"tight_niso\" : ak.firsts(truthsel & photon_preselection & photon_selections[\"tight\"]  & photon_selections[\"niso\"]),\n",
    "            \"ntight_iso\" : ak.firsts(truthsel & photon_preselection & photon_selections[\"ntight\"] & photon_selections[\"iso\"] ),\n",
    "            \"ntight_niso\": ak.firsts(truthsel & photon_preselection & photon_selections[\"ntight\"] & photon_selections[\"niso\"]),\n",
    "        }\n",
    "\n",
    "        h_ph_pt = (\n",
    "            had.Hist.new.StrCat([\"A\",\"B\",\"C\",\"D\"], name=\"ABCD\")\n",
    "            .Regular(100, 0.0, 100.0, name=\"pt\", label=\"$pt_{\\\\gamma}$ [GeV]\")\n",
    "            .Int64()\n",
    "        )\n",
    "\n",
    "        h_ph_pt.fill(ABCD=\"A\", pt=ak.firsts(events[SR & ABCD_selections[\"tight_niso\" ]].ph.pt) / 1.0e3)\n",
    "        h_ph_pt.fill(ABCD=\"B\", pt=ak.firsts(events[SR & ABCD_selections[\"ntight_niso\"]].ph.pt) / 1.0e3)\n",
    "        h_ph_pt.fill(ABCD=\"C\", pt=ak.firsts(events[SR & ABCD_selections[\"tight_iso\"  ]].ph.pt) / 1.0e3)\n",
    "        h_ph_pt.fill(ABCD=\"D\", pt=ak.firsts(events[SR & ABCD_selections[\"ntight_iso\" ]].ph.pt) / 1.0e3)\n",
    "\n",
    "        return {\n",
    "            dataset: {\n",
    "                \"entries\": ak.num(events, axis=0),\n",
    "                \"ph_pt\": h_ph_pt,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d1f716-ab1b-4995-a39f-15b0f35b7e82",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 28 column 5 (char 814)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m my_processor \u001b[38;5;241m=\u001b[39m MyProcessor()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# load in a bunch of datasets\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#dataset_runnable = json.loads(Path(\"/data/mhance/light-roast-main/dataset_runnable/af_v2.json\").read_text())\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m dataset_runnable \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maf_v2.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m cluster\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m dataset_to_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pixi/envs/default/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.pixi/envs/default/lib/python3.12/json/decoder.py:338\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.pixi/envs/default/lib/python3.12/json/decoder.py:354\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 28 column 5 (char 814)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "my_processor = MyProcessor()\n",
    "\n",
    "# load in a bunch of datasets\n",
    "#dataset_runnable = json.loads(Path(\"/data/mhance/light-roast-main/dataset_runnable/af_v2.json\").read_text())\n",
    "dataset_runnable = json.loads(Path(\"af_v2.json\").read_text())\n",
    "\n",
    "cluster=None\n",
    "dataset_to_run=None\n",
    "\n",
    "can_submit_to_condor=False\n",
    "datasettag='Znunugamma'\n",
    "\n",
    "if can_submit_to_condor:\n",
    "    # To facilitate usage with HTCondor\n",
    "    cluster = HTCondorCluster(\n",
    "        log_directory=Path().cwd() / \".condor_logs\" / \"cutflows_v2\",\n",
    "        cores=4,\n",
    "        memory=\"4GB\",\n",
    "        disk=\"2GB\",\n",
    "    )\n",
    "    cluster.scale(jobs=100)\n",
    "\n",
    "    # if we're running over all samples, ensure that here\n",
    "    dataset_to_run=dataset_runnable\n",
    "else:\n",
    "    cluster=LocalCluster()\n",
    "    dataset_to_run={datasettag: dataset_runnable[datasettag]}\n",
    "\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"Applying to fileset\")\n",
    "out = apply_to_fileset(\n",
    "    my_processor,\n",
    "    dataset_to_run,\n",
    "    schemaclass=NtupleSchema,\n",
    ")\n",
    "\n",
    "print(\"Beginning of dask.compute()\")\n",
    "\n",
    "# Add progress bar for dask\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "(computed,) = dask.compute(out)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution time: \", end_time - start_time)\n",
    "print(\"Finished dask.compute\")\n",
    "print(computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40e148-7e26-4cd8-bce0-44094c78a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "fig, ax = plt.subplots()\n",
    "computed[datasettag][datasettag][\"ph_pt\"].plot1d(ax=ax)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(title=\"Photon pT for $\\\\gamma + W\\\\rightarrow e \\\\nu$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f06e8d-cfad-4687-93f6-65f5a3b61a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62526d5f-10ad-4902-b2a8-b007b5f9b475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "light-roast-kernel",
   "language": "python",
   "name": "light-roast-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
