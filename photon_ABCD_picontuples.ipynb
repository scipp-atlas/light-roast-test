{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e4332a-a64f-4945-9194-49867bf76ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os #for looping over files in a directory\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.integer,)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating,)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        return super().default(obj)\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads JSON data from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict or list: A Python dictionary or list representing the JSON data, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{file_path}'\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in '{file_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "         print(f\"An unexpected error occurred: {e}\")\n",
    "         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ae5900-b1ea-4ac7-8f61-1b8571b68d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cuts, like mindphijetmet, nbjet=0, nlepton=0, already applied at preselection level for now\n",
    "SRcuts={'met_met': 200,\n",
    "        'mTGammaMet': 50,\n",
    "        'j1_pt': 150,\n",
    "        'ph_pt': 10,\n",
    "        'met_signif': 25}\n",
    "\n",
    "def getemptyresults():\n",
    "    results={}\n",
    "    for b in ['TT', 'TL', 'LT', 'LL']:\n",
    "        results[b] = {'real': {'nevents': 0,\n",
    "                               'sumweights': 0},\n",
    "                      'fake': {'nevents': 0,\n",
    "                               'sumweights': 0},\n",
    "                      'other': {'nevents': 0,\n",
    "                                'sumweights': 0},\n",
    "                      'data': 0\n",
    "                     }\n",
    "    return results\n",
    "\n",
    "def ABCDresults(data,mask,isMC):\n",
    "    masks={}\n",
    "    masks['TT'] = (data['ph_select_tightID']==1) & (data['ph_select_tightIso']==1) & ((data['ph_isEM'] & 0x45fc01)==0)\n",
    "    masks['TL'] = (data['ph_select_tightID']==1) & (data['ph_select_tightIso']==0) & ((data['ph_isEM'] & 0x45fc01)==0)\n",
    "    masks['LT'] = (data['ph_select_tightID']==0) & (data['ph_select_tightIso']==1) & ((data['ph_isEM'] & 0x45fc01)==0)\n",
    "    masks['LL'] = (data['ph_select_tightID']==0) & (data['ph_select_tightIso']==0) & ((data['ph_isEM'] & 0x45fc01)==0)\n",
    "\n",
    "    if isMC:\n",
    "        real_mask = (data['ph_truthprompt'] == 1)\n",
    "        fake_mask = (data['ph_truthJFP']    == 1)\n",
    "\n",
    "    results=getemptyresults()\n",
    "    for b in ['TT', 'TL', 'LT', 'LL']:\n",
    "        if isMC:\n",
    "            results[b]['real']['nevents']     = np.sum(mask & masks[b] & real_mask)\n",
    "            results[b]['real']['sumweights']  = np.sum(data['weight_total'][mask & masks[b] & real_mask])\n",
    "            results[b]['fake']['nevents']     = np.sum(mask & masks[b] & fake_mask)\n",
    "            results[b]['fake']['sumweights']  = np.sum(data['weight_total'][mask & masks[b] & fake_mask])\n",
    "            results[b]['other']['nevents']    = np.sum(mask & masks[b] & ~real_mask & ~fake_mask)\n",
    "            results[b]['other']['sumweights'] = np.sum(data['weight_total'][mask & masks[b] & ~real_mask & ~fake_mask])\n",
    "        else:\n",
    "            results[b]['data'] = np.sum(mask & masks[b])\n",
    "    return results\n",
    "\n",
    "def dumpjson(data,isMC):\n",
    "\n",
    "    PS={}\n",
    "    PS['0L'] = \\\n",
    "    (data['met_met']         >  SRcuts['met_met']*1000.   ) & \\\n",
    "    (data['j1_pt']           >  SRcuts['j1_pt']*1000.     ) & \\\n",
    "    (data['ph_pt']           >  SRcuts['ph_pt']*1000.     ) & \\\n",
    "    (data['nBTagJets']       == 0                         ) & \\\n",
    "    (data['mindPhiJetMet']   >  0.4                       ) & \\\n",
    "    (data['nElectrons']      == 0                         ) & \\\n",
    "    (data['nMuons']          == 0                         ) & \\\n",
    "    (data['nTau20_baseline'] == 0                         )\n",
    "\n",
    "    \n",
    "    SR={}\n",
    "    SR['0L-mT-low'] = PS['0L'] & \\\n",
    "    (data['mTGammaMet']      <  50.*1000.) & \\\n",
    "    (data['met_signif']      >  25       ) & \\\n",
    "    (data['mindPhiGammaJet'] >  1.5      )\n",
    "\n",
    "    SR['0L-mT-mid'] = PS['0L'] & \\\n",
    "    (data['mTGammaMet']      >   50*1000.) & \\\n",
    "    (data['mTGammaMet']      <  115*1000.) & \\\n",
    "    (data['met_signif']      >  20       ) & \\\n",
    "    (data['mindPhiGammaJet'] >  1.5      ) & \\\n",
    "    (data['dPhiGammaJ1']     >  1.5       )\n",
    "\n",
    "    SR['0L-mT-hgh'] = PS['0L'] & \\\n",
    "    (data['mTGammaMet']      >  115*1000.) & \\\n",
    "    (data['met_signif']      >  15       ) & \\\n",
    "    (data['mindPhiGammaJet'] >  1.5      ) & \\\n",
    "    (data['dPhiGammaJ1']     >  1.5      )\n",
    "\n",
    "    VR={}\n",
    "    VR['0L-mT-mid'] = PS['0L'] & \\\n",
    "    (data['mTGammaMet']      >   50*1000.) & \\\n",
    "    (data['mTGammaMet']      <  115*1000.) & \\\n",
    "    (data['dPhiGammaMet']    >  2.0      )\n",
    "    #(data['mindPhiGammaJet'] <  1.0      )\n",
    "    \n",
    "    \n",
    "    return {'SR': {'0L-mT-low': ABCDresults(data, SR['0L-mT-low'], isMC),\n",
    "                   '0L-mT-mid': ABCDresults(data, SR['0L-mT-mid'], isMC),\n",
    "                   '0L-mT-hgh': ABCDresults(data, SR['0L-mT-hgh'], isMC),\n",
    "                  },\n",
    "            'VR': {'0L-mT-mid': ABCDresults(data, VR['0L-mT-mid'], isMC),\n",
    "                  },\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14dcd0e-475d-4a67-b379-aa06554341dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_path = \"/data/mhance/SUSY/ntuples/v3\"\n",
    "\n",
    "# Iterate over subdirectories and files\n",
    "for root, _, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        if not file.endswith('.root'): continue\n",
    "        filepath = os.path.join(root, file)\n",
    "        #if filepath != \"/data/mhance/SUSY/ntuples/v3/output_Wtaunugamma.root\": continue\n",
    "        #if filepath != \"/data/mhance/SUSY/ntuples/v3/output_data_2018.root\": continue\n",
    "        #if filepath != \"/data/mhance/SUSY/ntuples/v3/output_Znunu_CVetoBVeto.root\": continue\n",
    "        #if filepath != \"/data/mhance/SUSY/ntuples/v3/output_N2_220_N1_200_HH.root\": continue\n",
    "        #print(filepath)\n",
    "        with uproot.open(filepath) as f:\n",
    "            if 'picontuple' in f:\n",
    "                tree = f['picontuple']\n",
    "                # Extract the data\n",
    "                data = tree.arrays(library=\"np\")\n",
    "                #data['met_signif'] = data['met_met']/data['ph_pt']\n",
    "\n",
    "                results=dumpjson(data,\"data_\" not in filepath)\n",
    "                #print(json.dumps(results, indent=4, cls=NumpyEncoder))\n",
    "                with open(\"ABCD_results/\"+file.replace(\".root\",\"_ABCD.json\"),'w') as jf:\n",
    "                    json.dump(results, jf, indent=4, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d6c011-5d62-4eb1-bbf1-5c100efaf9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getfakeestimate(regiontype=\"SR\",regionname=\"0L-mT-low\",debug=False):\n",
    "    \n",
    "    totalresults=getemptyresults()\n",
    "    sample_max={}\n",
    "    sample_max['TL']=[0,'']\n",
    "    sample_max['LT']=[0,'']\n",
    "    sample_max['TT']=[0,'']\n",
    "    sample_max['LL']=[0,'']\n",
    "    \n",
    "    samples=[]\n",
    "    \n",
    "    for fp in glob.glob(\"ABCD_results/*.json\"):\n",
    "        if \"gammajet\" in fp: continue\n",
    "        if \"jetjet\" in fp: continue\n",
    "        if \"N2\" in fp: continue\n",
    "            \n",
    "        data = load_json_file(fp)\n",
    "    \n",
    "        sample_tag = fp.replace(\"_results/output_\",\"\").replace(\"ABCD\",\"\").replace(\".json\",\"\")[:-1]\n",
    "        samples.append(sample_tag)\n",
    "\n",
    "        region=data[regiontype][regionname]\n",
    "        \n",
    "        for b in ['TT', 'TL', 'LT', 'LL']:\n",
    "            totalresults[b]['data'] += region[b][\"data\"]\n",
    "            totalresults[b]['real']['sumweights'] += region[b][\"real\"][\"sumweights\"]\n",
    "            totalresults[b]['fake']['sumweights'] += region[b][\"fake\"][\"sumweights\"]\n",
    "            totalresults[b]['other']['sumweights'] += region[b][\"other\"][\"sumweights\"]\n",
    "    \n",
    "            if sample_max[b][0] < region[b][\"real\"][\"sumweights\"]:\n",
    "                sample_max[b][0] = region[b][\"real\"][\"sumweights\"]\n",
    "                sample_max[b][1] = sample_tag\n",
    "                \n",
    "    if debug:\n",
    "        print(json.dumps(totalresults,indent=4,cls=NumpyEncoder))\n",
    "\n",
    "        print(\"Most contributing samples:\")\n",
    "        for b in ['TT', 'TL', 'LT', 'LL']:\n",
    "            print(f\"{b}: {sample_max[b][1][:-1]}\")\n",
    "        \n",
    "        mcs_data=load_json_file(f\"ABCD_results/output_{sample_max['TT'][1]}_ABCD.json\")\n",
    "        print(json.dumps(mcs_data,indent=4,cls=NumpyEncoder))\n",
    "\n",
    "    return totalresults,samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3eafe0-cb12-4443-a55c-f57163799afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#totalresults=getfakeestimate(\"SR\",\"0L-mT-low\",False)\n",
    "regiontype=\"VR\"\n",
    "blindTT = (regiontype == \"SR\")\n",
    "totalresults,samples=getfakeestimate(regiontype,\"0L-mT-mid\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41d66d7-bb37-4275-9d88-d441227be67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TT: 1746.0    262.9\n",
      "TL: 2292.0    134.8\n",
      "LT: 1208.0     40.1\n",
      "LL: 1966.0     26.0\n",
      "N_TT_bkg = (2157.2*1167.9)/(1940.0) = 1298.6, N_TT_fake=140.4, N_TT_other=508.7, and N_TT_real=262.9\n",
      "Total data in TT region is 1746.0.\n",
      "DD background prediction: 262.9 (real) + 1298.6 (fake) + 508.7 (other) = 2070.2\n",
      "MC background prediction: 262.9 (real) + 140.4 (fake) + 508.7 (other) = 912.0\n"
     ]
    }
   ],
   "source": [
    "#N_LL = totalresults['LL']['data']-totalresults['LL']['real']['sumweights']\n",
    "#N_TL = totalresults['TL']['data']-totalresults['TL']['real']['sumweights']\n",
    "#N_LT = totalresults['LT']['data']-totalresults['LT']['real']['sumweights']\n",
    "\n",
    "N={}\n",
    "for b in ['TT', 'TL', 'LT', 'LL']:\n",
    "    N[b] = totalresults[b]['data']-totalresults[b]['real']['sumweights']\n",
    "    if b != 'TT' or (not blindTT):\n",
    "        print(f\"{b}: {totalresults[b]['data']:6.1f}   {totalresults[b]['real']['sumweights']:6.1f}\")\n",
    "\n",
    "if N['LL']>0:\n",
    "    N_TT_bkg_DDfake = N['TL']*N['LT']/N['LL']\n",
    "else:\n",
    "    N_TT_bkg_DDfake = 0\n",
    "\n",
    "N_TT_bkg_real = totalresults['TT']['real']['sumweights']\n",
    "N_TT_bkg_other = totalresults['TT']['other']['sumweights']\n",
    "N_TT_bkg_MCfake = totalresults['TT']['fake']['sumweights']\n",
    "\n",
    "N_TT_bkg_MC = N_TT_bkg_MCfake + N_TT_bkg_real + N_TT_bkg_other\n",
    "\n",
    "N_TT_bkg_DD = N_TT_bkg_DDfake + N_TT_bkg_real + N_TT_bkg_other\n",
    "\n",
    "print(f\"N_TT_bkg = ({N['TL']:.1f}*{N['LT']:.1f})/({N['LL']:.1f}) = {N_TT_bkg_DDfake:.1f}, N_TT_fake={N_TT_bkg_MCfake:.1f}, N_TT_other={N_TT_bkg_other:.1f}, and N_TT_real={N_TT_bkg_real:.1f}\")\n",
    "\n",
    "if not blindTT:\n",
    "    print(f\"Total data in TT region is {totalresults['TT']['data']:.1f}.\")\n",
    "    print(f\"DD background prediction: {N_TT_bkg_real:5.1f} (real) + {N_TT_bkg_DDfake:.1f} (fake) + {N_TT_bkg_other:.1f} (other) = {N_TT_bkg_DD:.1f}\")\n",
    "    print(f\"MC background prediction: {N_TT_bkg_real:5.1f} (real) + {N_TT_bkg_MCfake:.1f} (fake) + {N_TT_bkg_other:.1f} (other) = {N_TT_bkg_MC:.1f}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25ecb7-dcd5-4447-a892-65abda17b9ff",
   "metadata": {},
   "source": [
    "Quick function that will test closure for any single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeaa13fe-48af-4aed-bf6d-152cc6edaf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleABCD(sample,debug=False):\n",
    "    sresults=None\n",
    "    if isinstance(sample,str):\n",
    "        sresults=load_json_file(f\"ABCD_results/output_{sample}_ABCD.json\")[\"VR\"][\"0L-mT-mid\"]\n",
    "        #print(json.dumps(results,indent=4,cls=NumpyEncoder))\n",
    "    elif isinstance(sample,dict):\n",
    "        sresults=sample\n",
    "    else:\n",
    "        print(\"Must provide either valid sample string or dictionary of results.\")\n",
    "        return None\n",
    "\n",
    "    N_all={}\n",
    "    for b in ['TT', 'TL', 'LT', 'LL']:\n",
    "        N_all[b] = sresults[b]['real']['sumweights']+sresults[b]['fake']['sumweights']\n",
    "\n",
    "    num_TL = (N_all['TL']-sresults['TL']['real']['sumweights'])\n",
    "    num_LT = (N_all['LT']-sresults['LT']['real']['sumweights'])\n",
    "    den_LL = (N_all['LL']-sresults['LL']['real']['sumweights'])\n",
    "    N_TT_fake_est = 0.\n",
    "    if den_LL > 0:\n",
    "        N_TT_fake_est = num_TL*num_LT/den_LL\n",
    "\n",
    "    if debug and den_LL > 0 and sresults['TT']['fake']['sumweights']>0:\n",
    "        print(f\"{sample:52s} {sresults['TT']['real']['sumweights']:6.1f}  {N_TT_fake_est:6.1f}  {sresults['TT']['fake']['sumweights']:6.1f}  {sresults['TT']['other']['sumweights']:6.1f}   {(N_TT_fake_est-sresults['TT']['fake']['sumweights'])/sresults['TT']['fake']['sumweights']:6.1f}\")\n",
    "    elif debug:\n",
    "        print(f\"{sample:52s} {sresults['TT']['real']['sumweights']:6.1f}  {N_TT_fake_est:6.1f}  {sresults['TT']['fake']['sumweights']:6.1f}  {sresults['TT']['other']['sumweights']:6.1f}\")\n",
    "        \n",
    "    return N_TT_fake_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d95c22a3-aa9b-4f49-b7d1-48a2364fd975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample                                               Prompt    ABCD   MC fakes  EFP/other   (ABCD-MC)/MC\n",
      "Ztautaugamma                                            0.6     0.0    -0.0     0.0\n",
      "Wtaunu_L_BFilter                                        0.0     0.3     0.1     0.9      1.9\n",
      "Sh_2211_ZbbZvv                                          0.0     0.0     0.0     0.0      0.2\n",
      "Wtaunu_H_CFilterBVeto                                   0.0     3.5     2.2    11.1      0.6\n",
      "Sh_2212_lllv                                            0.0     0.0     0.0     0.0     -0.9\n",
      "Wenugamma                                              16.9     0.2     0.1     1.0      0.1\n",
      "Sh_2212_llvv_ss                                         0.0     0.0     0.0     0.0     -0.9\n",
      "Znunu_BFilter                                          -0.0     1.9     2.9     6.4     -0.3\n",
      "Sh_2211_Ztautau_LL_maxHTpTV2_CFilterBVeto               0.0     0.0     0.0     0.0     -0.4\n",
      "Zmumugamma                                              0.7     0.0     0.0     0.1     -0.3\n",
      "Sh_2211_Zmumu_maxHTpTV2_BFilter                         0.0     0.0     0.0     0.0      2.6\n",
      "Sh_2212_lvvv                                            0.0     0.2     0.2     0.5      0.0\n",
      "Sh_2211_WlvZbb                                          0.0     0.0     0.0     0.0\n",
      "Wtaunu_H_CVetoBVeto                                     0.0    11.9    27.1    58.2     -0.6\n",
      "Sh_2211_WqqZvv                                          0.0     0.6     0.9     1.2     -0.4\n",
      "Znunu_CFilterBVeto                                      0.0    20.1    18.9    51.9      0.1\n",
      "Wenu_CVetoBVeto                                         0.9     5.4     5.9    56.2     -0.1\n",
      "Sh_2211_WlvWqq                                          0.0     0.5     0.3     0.8      0.7\n",
      "Wmunu_CFilterBVeto                                      0.0     2.5     3.3     6.9     -0.2\n",
      "Sh_2211_Zmumu_maxHTpTV2_CFilterBVeto                    0.0     0.0     0.1     0.1     -0.4\n",
      "Sh_2211_Ztautau_HH_maxHTpTV2_CFilterBVeto               0.0     0.0     0.0     0.0\n",
      "ttbar                                                   0.0     0.9     1.6     2.3     -0.5\n",
      "Znunugamma                                            181.7     0.8     0.8     3.1      0.1\n",
      "Wtaunugamma                                            45.2     0.6     0.6     2.5      0.1\n",
      "Sh_2211_WlvZqq                                          0.0     0.0     0.1     0.4     -0.9\n",
      "PhPy8EG_A14_tchan_BW50_lept_top                         0.0     0.3     0.3     0.4      0.2\n",
      "Wtaunu_H_BFilter                                        0.0     0.7     0.1     1.4      6.2\n",
      "Wenu_CFilterBVeto                                       0.0     2.6     1.5    11.6      0.6\n",
      "Wmunu_BFilter                                           0.0     0.5     0.6     1.0     -0.2\n",
      "Wmunugamma                                             16.1     0.2     0.2     1.0     -0.1\n",
      "PhPy8EG_tW_dyn_DR_incl_antitop                          0.0     0.1     0.0     0.2\n",
      "Sh_2211_Ztautau_LL_maxHTpTV2_BFilter                    0.0     0.0     0.0     0.0     -1.0\n",
      "Sh_2211_Ztautau_LL_maxHTpTV2_CVetoBVeto                 0.0     0.0     0.0     0.1      1.3\n",
      "Sh_2212_llvvjj_ss                                       0.0     0.0     0.0     0.0\n",
      "Sh_2212_vvvv                                            0.0     0.0     0.0     0.1      0.7\n",
      "Wtaunu_L_CFilterBVeto                                   0.0     2.7     1.8     3.3      0.5\n",
      "Wtaunu_L_CVetoBVeto                                     0.0     9.1     6.1    25.7      0.5\n",
      "Wmunu_CVetoBVeto                                        0.0     5.0     6.7    36.7     -0.3\n",
      "PhPy8EG_tW_dyn_DR_incl_top                              0.0     0.4     0.0     0.4\n",
      "Znunu_CVetoBVeto                                        0.1    64.2    56.1   219.5      0.1\n",
      "Sh_2212_llvv_os                                         0.1     0.1     0.1     0.3     -0.2\n",
      "Sh_2211_ZqqZvv                                          0.0     0.1     0.3     0.7     -0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"{\"Sample\":52s} {\"Prompt\":6s}    {\"ABCD\":6s} {\"MC fakes\":6s}  {\"EFP/other\":6s}   {\"(ABCD-MC)/MC\":6s}\")\n",
    "for s in samples:\n",
    "    est=sampleABCD(s,False)\n",
    "    if est>0.:\n",
    "        sampleABCD(s,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc08b1-ba1b-4580-a5a9-393ab73f6d20",
   "metadata": {},
   "source": [
    "This seems to be working.  To do:\n",
    "* Implement some other regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c3962-f34e-4bf1-aa68-f8a00d3bde8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd28fb-7ea8-4b92-985f-885341d887a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "light-roast-kernel",
   "language": "python",
   "name": "light-roast-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
