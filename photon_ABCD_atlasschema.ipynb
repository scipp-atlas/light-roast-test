{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc7cdbf-cf03-4f80-948c-1c2c7884306e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this portion is done to ignore warnings from coffea for now\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import parse\n",
    "import atlas_schema\n",
    "\n",
    "import coffea\n",
    "\n",
    "from atlas_schema.methods import behavior as as_behavior\n",
    "from atlas_schema.schema import NtupleSchema\n",
    "from coffea import processor\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea.dataset_tools import apply_to_fileset\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue.htcondor import HTCondorCluster\n",
    "from dask.distributed import LocalCluster\n",
    "from matplotlib import pyplot as plt\n",
    "import hist.dask as had\n",
    "\n",
    "fname_pattern = parse.compile(\n",
    "    \"user.{username:w}.{dsid:d}.{process:S}.{campaign:w}.v{version:.1f}_ANALYSIS.root\"\n",
    ")\n",
    "\n",
    "colors_dict = {\n",
    "    \"Znunu\": \"b\",\n",
    "    \"Wenu\": \"g\",\n",
    "    \"Wmunu\": \"r\",\n",
    "    \"Wtaunu_L\": \"c\",\n",
    "    \"Wtaunu_H\": \"m\",\n",
    "    \"Znunugamma\": \"y\",\n",
    "    \"Wmunugamma\": \"k\",\n",
    "    \"Wenugamma\": \"brown\",\n",
    "    \"Wtaunugamma\": \"pink\",\n",
    "    \"N2_100_N1_97_WB_signal\": \"rosybrown\",\n",
    "    \"Fake/Nonprompt\": \"lime\",\n",
    "}  #  'slategrey', 'blueviolet', 'crimson'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"coffea.*\")\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f4a3d-fec8-4e22-ad6c-364c2cf6cc81",
   "metadata": {},
   "source": [
    "Now create a processor that will handle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bab73c1-a262-459b-b2f7-d80859abf41f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        # can define histograms here\n",
    "        pass\n",
    "\n",
    "    def process(self, events):\n",
    "        ## TODO: remove this temporary fix when https://github.com/scikit-hep/vector/issues/498 is resolved\n",
    "        met_dict = {field: events.met[field] for field in events.met.fields}\n",
    "        met_dict[\"pt\"] = dak.zeros_like(events.met.met)\n",
    "        met_dict[\"eta\"] = dak.zeros_like(events.met.met)\n",
    "        events[\"met\"] = dak.zip(met_dict, with_name=\"MissingET\", behavior=as_behavior)\n",
    "\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        \n",
    "        print(f\"processing {len(events)} events for {dataset}\")\n",
    "        # xs = events.metadata[\"xs\"]\n",
    "        # lum = events.metadata[\"luminosity\"]\n",
    "        # process = events.metadata[\"process\"]\n",
    "        # genFiltEff = events.metadata[\"genFiltEff\"]\n",
    "        # evt_count = ak.num(events, axis=0).compute()\n",
    "        # weights = (xs * genFiltEff * lum / evt_count) * np.ones(evt_count)\n",
    "\n",
    "        leptons = ak.concatenate((events.el, events.mu), axis=1)\n",
    "\n",
    "        # here are some selection cuts for something that looks like the signal region.\n",
    "        # the only thing that's different is the MET requirement, which I inverted to be\n",
    "        # met<250 instead of met>250, to make sure we don't accidentally unblind the SR\n",
    "        # and to give us some more stats while we study MC samples.\n",
    "        selections = {\n",
    "            \"met\": (events.met.met < 250 * 1.0e3),\n",
    "            \"lepton_veto\": (ak.sum(leptons.pt, axis=1) == 0),\n",
    "            \"leading_jet_pt\": (ak.firsts(events.jet.pt) > 100 * 1.0e3),\n",
    "            \"min_dphi_jet_met\": (ak.min(abs(events.met.delta_phi(events.jet)), axis=1) > 0.4),\n",
    "            \"bjet_veto\": (ak.sum(events.jet.btag_select, axis=1) == 0),\n",
    "            \"vgamma_overlap\": (events[\"in\"][\"vgamma_overlap_7\"]==1),\n",
    "        }\n",
    "        \n",
    "        selection = PackedSelection()\n",
    "        selection.add_multiple(selections)\n",
    "\n",
    "        SR=(selection.all())\n",
    "        presel_events=events[SR]\n",
    "        \n",
    "        # photon object preselection\n",
    "        ph_preselection = (\n",
    "            (presel_events.ph.pt>10000) &\n",
    "            (presel_events.ph.select_baseline==1) &\n",
    "            ((presel_events.ph.isEM&0x45fc01)==0) &\n",
    "            (\n",
    "             (abs(presel_events.ph.eta)<1.37) | \n",
    "             ((abs(presel_events.ph.eta)>1.52) & \n",
    "              (abs(presel_events.ph.eta)<2.37))\n",
    "            ) &\n",
    "            (presel_events.ph.select_or_dR02Ph==1)\n",
    "        )\n",
    "\n",
    "        # this selects events with at least one baseline photon\n",
    "        ph_presel_data=presel_events[ak.any(ph_preselection,axis=1)]\n",
    "\n",
    "        # define tight and loose cuts, now on the smaller data sample that only has good events\n",
    "        ph_preselection=((ph_presel_data.ph.pt>10000) & \n",
    "                         ((abs(ph_presel_data.ph.eta)<1.37) | ((abs(ph_presel_data.ph.eta)>1.52) & \n",
    "                                                               (abs(ph_presel_data.ph.eta)<2.37))) &\n",
    "                         (ph_presel_data.ph.select_or_dR02Ph==1) &\n",
    "                         ((ph_presel_data.ph.isEM&0x45fc01)==0) &\n",
    "                         (ph_presel_data.ph.select_baseline==1)\n",
    "                        )\n",
    "\n",
    "        # get the index of the first preselected photon (which should be the leading preselected photon)\n",
    "        indices=ak.argmax(ph_preselection,axis=1,keepdims=True)\n",
    "        \n",
    "        # apply cuts to that index\n",
    "        ph_tight = (ak.firsts(ph_presel_data.ph[indices].select_tightID)==1)\n",
    "        ph_iso   = (ak.firsts(ph_presel_data.ph[indices].select_tightIso)==1)\n",
    "        ph_truth = ((ak.firsts(ph_presel_data.ph[indices].truthType) != 0) & \n",
    "                    (ak.firsts(ph_presel_data.ph[indices].truthType) != 16))\n",
    "\n",
    "        return {\n",
    "            dataset: {\n",
    "                \"entries\": ak.num(events, axis=0)\n",
    "            },\n",
    "            \"presel\": {\n",
    "                \"total\": ak.num(presel_events,axis=0),\n",
    "                \"met\": presel_events.met.met,\n",
    "                \"njets\": ak.num(presel_events.jet.pt,axis=1)\n",
    "            },\n",
    "            \"ABCD\": {\n",
    "                \"A_true\": ak.num(ph_presel_data.ph[indices].pt[ ph_tight & ~ph_iso &  ph_truth][:,0],axis=0),\n",
    "                \"B_true\": ak.num(ph_presel_data.ph[indices].pt[~ph_tight & ~ph_iso &  ph_truth][:,0],axis=0),\n",
    "                \"C_true\": ak.num(ph_presel_data.ph[indices].pt[ ph_tight &  ph_iso &  ph_truth][:,0],axis=0),\n",
    "                \"D_true\": ak.num(ph_presel_data.ph[indices].pt[~ph_tight &  ph_iso &  ph_truth][:,0],axis=0),\n",
    "                \"A_fake\": ak.num(ph_presel_data.ph[indices].pt[ ph_tight & ~ph_iso & ~ph_truth][:,0],axis=0),\n",
    "                \"B_fake\": ak.num(ph_presel_data.ph[indices].pt[~ph_tight & ~ph_iso & ~ph_truth][:,0],axis=0),\n",
    "                \"C_fake\": ak.num(ph_presel_data.ph[indices].pt[ ph_tight &  ph_iso & ~ph_truth][:,0],axis=0),\n",
    "                \"D_fake\": ak.num(ph_presel_data.ph[indices].pt[~ph_tight &  ph_iso & ~ph_truth][:,0],axis=0),\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d1f716-ab1b-4995-a39f-15b0f35b7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying to fileset\n",
      "processing 508801 events for Znunugamma\n",
      "Beginning of dask.compute()\n",
      "[########################################] | 100% Completed | 206.11 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhance/.pixi/envs/default/lib/python3.12/site-packages/coffea/nanoevents/schemas/fcc.py:5: FutureWarning: In version 2025.1.0 (target date: 2024-12-31 11:59:59-06:00), this will be an error.\n",
      "To raise these warnings as errors (and get stack traces to find out where they're called), run\n",
      "    import warnings\n",
      "    warnings.filterwarnings(\"error\", module=\"coffea.*\")\n",
      "after the first `import coffea` or use `@pytest.mark.filterwarnings(\"error:::coffea.*\")` in pytest.\n",
      "Issue: coffea.nanoevents.methods.vector will be removed and replaced with scikit-hep vector. Nanoevents schemas internal to coffea will be migrated. Otherwise please consider using that package!.\n",
      "  from coffea.nanoevents.methods import vector\n",
      "/home/mhance/.pixi/envs/default/lib/python3.12/site-packages/coffea/nanoevents/schemas/fcc.py:5: FutureWarning: In version 2025.1.0 (target date: 2024-12-31 11:59:59-06:00), this will be an error.\n",
      "To raise these warnings as errors (and get stack traces to find out where they're called), run\n",
      "    import warnings\n",
      "    warnings.filterwarnings(\"error\", module=\"coffea.*\")\n",
      "after the first `import coffea` or use `@pytest.mark.filterwarnings(\"error:::coffea.*\")` in pytest.\n",
      "Issue: coffea.nanoevents.methods.vector will be removed and replaced with scikit-hep vector. Nanoevents schemas internal to coffea will be migrated. Otherwise please consider using that package!.\n",
      "  from coffea.nanoevents.methods import vector\n",
      "/home/mhance/.pixi/envs/default/lib/python3.12/site-packages/coffea/nanoevents/schemas/fcc.py:5: FutureWarning: In version 2025.1.0 (target date: 2024-12-31 11:59:59-06:00), this will be an error.\n",
      "To raise these warnings as errors (and get stack traces to find out where they're called), run\n",
      "    import warnings\n",
      "    warnings.filterwarnings(\"error\", module=\"coffea.*\")\n",
      "after the first `import coffea` or use `@pytest.mark.filterwarnings(\"error:::coffea.*\")` in pytest.\n",
      "Issue: coffea.nanoevents.methods.vector will be removed and replaced with scikit-hep vector. Nanoevents schemas internal to coffea will be migrated. Otherwise please consider using that package!.\n",
      "  from coffea.nanoevents.methods import vector\n",
      "/home/mhance/.pixi/envs/default/lib/python3.12/site-packages/coffea/nanoevents/schemas/fcc.py:5: FutureWarning: In version 2025.1.0 (target date: 2024-12-31 11:59:59-06:00), this will be an error.\n",
      "To raise these warnings as errors (and get stack traces to find out where they're called), run\n",
      "    import warnings\n",
      "    warnings.filterwarnings(\"error\", module=\"coffea.*\")\n",
      "after the first `import coffea` or use `@pytest.mark.filterwarnings(\"error:::coffea.*\")` in pytest.\n",
      "Issue: coffea.nanoevents.methods.vector will be removed and replaced with scikit-hep vector. Nanoevents schemas internal to coffea will be migrated. Otherwise please consider using that package!.\n",
      "  from coffea.nanoevents.methods import vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  573.7915887832642\n",
      "Finished dask.compute\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "my_processor = MyProcessor()\n",
    "\n",
    "# load in a bunch of datasets\n",
    "#dataset_runnable = json.loads(Path(\"af_v2_2_mc.json\").read_text())\n",
    "dataset_runnable = json.loads(Path(\"af_v2_2_mc_onefile.json\").read_text())\n",
    "\n",
    "cluster=None\n",
    "dataset_to_run=None\n",
    "\n",
    "can_submit_to_condor=False\n",
    "datasettag='Znunugamma'\n",
    "\n",
    "if can_submit_to_condor:\n",
    "    # To facilitate usage with HTCondor\n",
    "    cluster = HTCondorCluster(\n",
    "        log_directory=Path().cwd() / \".condor_logs\" / \"cutflows_v2\",\n",
    "        cores=4,\n",
    "        memory=\"4GB\",\n",
    "        disk=\"2GB\",\n",
    "    )\n",
    "    cluster.scale(jobs=100)\n",
    "\n",
    "    # if we're running over all samples, ensure that here\n",
    "    dataset_to_run=dataset_runnable\n",
    "else:\n",
    "    cluster=LocalCluster()\n",
    "    dataset_to_run={datasettag: dataset_runnable[datasettag]}\n",
    "\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"Applying to fileset\")\n",
    "out = apply_to_fileset(\n",
    "    my_processor,\n",
    "    dataset_to_run,\n",
    "    schemaclass=NtupleSchema,\n",
    ")\n",
    "\n",
    "print(\"Beginning of dask.compute()\")\n",
    "\n",
    "# Add progress bar for dask\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "(computed,) = dask.compute(out)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution time: \", end_time - start_time)\n",
    "print(\"Finished dask.compute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44bd6b4e-b740-41b6-8f12-3df349359719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Znunugamma': {'Znunugamma': {'entries': 508801}, 'presel': {'total': np.int64(116881), 'met': <Array [2.26e+05, 2.08e+05, ..., 2.35e+05] type='116881 * float32[parameter...'>, 'njets': <Array [2, 2, 7, 2, 3, 4, 5, 2, ..., 2, 2, 3, 2, 2, 4, 3] type='116881 * int64'>}, 'ABCD': {'A_true': np.int64(24384), 'B_true': np.int64(2221), 'C_true': np.int64(65414), 'D_true': np.int64(3464), 'A_fake': np.int64(598), 'B_fake': np.int64(858), 'C_fake': np.int64(163), 'D_fake': np.int64(166)}}}\n",
      "{'total': np.int64(116881), 'met': <Array [2.26e+05, 2.08e+05, ..., 2.35e+05] type='116881 * float32[parameter...'>, 'njets': <Array [2, 2, 7, 2, 3, 4, 5, 2, ..., 2, 2, 3, 2, 2, 4, 3] type='116881 * int64'>}\n"
     ]
    }
   ],
   "source": [
    "print(computed)\n",
    "print(computed['Znunugamma']['presel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32dfd4-708e-41c8-8905-88a9bcc6dd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "light-roast-kernel",
   "language": "python",
   "name": "light-roast-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
